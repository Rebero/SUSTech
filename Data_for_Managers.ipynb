{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccRAVackyKLs"
      },
      "source": [
        "\n",
        "# Machine Learning Simulation with Python from A-Z\n",
        "## Author: René Eber\n",
        "\n",
        "**Confidential - Do not duplicate or distribute without written permission from the author**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83OVH_lm0SwI"
      },
      "source": [
        "# The Process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3RJifyUuaGv"
      },
      "source": [
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/process_empty_sh.png?alt=media&token=1168de47-e528-4dbd-aafe-58d2fe992110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wVVc-gbBVub"
      },
      "source": [
        "# Step 1: Start Line - Introduction to Business Case / Data-Set\n",
        "\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/ambulance.png?alt=media&token=1494f989-3014-495f-a69b-974275793bda)\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**Background:** You are the management / chief data scientists of an emergency response company that is responsible for all UK road-crash emergencies. It is your responsibility to:\n",
        "  -  send an ambulance to the location of the crash (e.g. a car crash) \n",
        "  - transport the injured people to the closest hospital\n",
        "  - provide medical emergency services to any people injured \n",
        "\n",
        "Generally, there are two different types of ambulances. They vary in the spectrum of medical emergency services they can facilitate:\n",
        "\n",
        "&emsp;\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/ambulances.png?alt=media&token=79e882b8-ad9f-4ebb-91df-ec06eeb1eb06)\n",
        "\n",
        "&emsp;\n",
        "\n",
        "The ambulance on the *left* only has the most basic medical equipment on board and is used for patients that are not critically injured.\n",
        "\n",
        "The ambulance on the *right* can be understood as an extension of the hospital, as there are doctors on board as well as sophisticated medical equipment. Even advanced medical procedures such as a surgery can be performed, while on the way to the hospital.\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**Objective:** You want to send the right type of ambulance as:\n",
        "  - you only have a limited amount of ambulances of both types\n",
        "  - sending an ambulance that includes a medical team is only needed for severe or fatal accidents and is much more costly\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**Problem:** You have to decide which ambulance to send, before you know about the severity of the accident. \n",
        "\n",
        "&emsp;\n",
        "\n",
        "**Solution idea:** We want to predict the accident severity from data available at the time the accident is reported. With these datapoints we want to create a predictor for the severity of the accident so that we can make a smart decision on which type of ambulance to send once we receive the emergency call.\n",
        "\n",
        "We will use the *publicly accessible dataset of UK crash data* to create the predcition model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnCZegCN0mcO"
      },
      "source": [
        "**Question: Which type of machine learning is this?**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/types_sh.png?alt=media&token=db812890-c7f9-4856-b0ed-4db12f30c57a)\n",
        "\n",
        "**Where we stand in the process & key tasks for you as a manager:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step1_sh.png?alt=media&token=01334c65-67de-4398-9067-ab3706caa87f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFfKOi4BGds"
      },
      "source": [
        "# Step 2: Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0-pYF6KIibd"
      },
      "source": [
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step2_before_sh.png?alt=media&token=8a3c8adb-04fd-491f-b0c7-a158026f5e69)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdE7agfm2DXx"
      },
      "source": [
        "First we import the \"libraries\" (e.g. pandas) that we are going to use in this workshop. These libraries contain all the functionalities that we are going to use (e.g. to calculate a mean, visualize data).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwPDwpXrzjzh"
      },
      "outputs": [],
      "source": [
        "# Loading libraries\n",
        "!pip install pandas\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install seaborn\n",
        "!pip install sklearn\n",
        "!pip install matplotlib\n",
        "!pip install imblearn\n",
        "!pip install scipy\n",
        "!pip install xlrd\n",
        "!pip install jupyterlab-language-pack-zh-CN\n",
        "!pip install ipywidgets\n",
        "\n",
        "# Loading libraries\n",
        "import os\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import uniform, randint\n",
        "import sklearn as sklearn\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import time\n",
        "sns.set()\n",
        "\n",
        "# Connect to data source\n",
        "! git clone https://github.com/Rebero/Quantgeist\n",
        "time.sleep(10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmNkMpoY2jMK"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "Now we are ready to import the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgfAYCAd2hut"
      },
      "outputs": [],
      "source": [
        "# import data from a comma seperated files (csv) file  \n",
        "crash_df = pd.read_csv('/content/Quantgeist/accidents_dataset_corrected.csv')\n",
        "\n",
        "crash_df[\"Accident_Severity_numerical\"] = crash_df[\"Accident_Severity\"]\n",
        "# Read from file with number descriptions and replace number with description where possible\n",
        "xls = pd.ExcelFile('/content/Quantgeist/Road-Accident-Safety-Data-Guide.xls')\n",
        "for name in xls.sheet_names: \n",
        "  if name != \"Introduction\" and name != \"Export Variables\":\n",
        "    names_df = pd.read_excel(xls, name)\n",
        "    if name == \"Ped Cross - Human\":\n",
        "      name = \"Pedestrian_Crossing-Human_Control\"\n",
        "    if name == \"Ped Cross - Physical\":\n",
        "      name = \"Pedestrian_Crossing-Physical_Facilities\"\n",
        "    if name == \"Weather\":\n",
        "      name = \"Weather_Conditions\"\n",
        "    if name == \"Road Surface\":\n",
        "      name = \"Road_Surface_Conditions\"\n",
        "    if name == \"Urban Rural\":\n",
        "      name = \"Urban_or_Rural_Area\"\n",
        "    if name == \"Police Officer Attend\":\n",
        "      name = \"Did_Police_Officer_Attend_Scene_of_Accident\"\n",
        "    name = name.replace(\" \", \"_\")\n",
        "    if name in list(crash_df.columns):\n",
        "      rename_dict = names_df.set_index('code').to_dict()['label']\n",
        "      crash_df[name] = crash_df[name].replace(rename_dict)\n",
        "\n",
        "# remove irrelevant columns\n",
        "crash_df = crash_df.drop(columns=['Location_Easting_OSGR', 'Location_Northing_OSGR', \"Local_Authority_(District)\", \"Local_Authority_(Highway)\", \"LSOA_of_Accident_Location\", \"Police_Force\", \"Accident_Index\", \"1st_Road_Number\",\t\"2nd_Road_Number\", \"Date\",\t\"Time\",\t\"Special_Conditions_at_Site\", \"Carriageway_Hazards\", \"Pedestrian_Crossing-Human_Control\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jkAKNuw_CRD"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "To get a first overview of the dataset, let us look at the dimensions of the data as e.g., the number of crashes it entails and the number of columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg7AkA4D_M81"
      },
      "outputs": [],
      "source": [
        "# dataframe.size returns the size of our dataset\n",
        "crash_df.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZL8BMF7KRCG"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Task: Here I want you to write the code yourself.** The objective is to look at the actual data. The method that you want to use is called **head()**. Remember our data-set is called **crash_df**. You want to do this analogously to how we looked at the shape of our dataset above invoking the method **shape** on our dataset crash_df right above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS-B8-jlB_hI"
      },
      "outputs": [],
      "source": [
        "#Your code goes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wPbPBoUIWLi"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "If we map the longitude and latitude values of each accident onto an empty coordinate system. **Question: What do you expect to see?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPw36f34IShb"
      },
      "outputs": [],
      "source": [
        "# lets map the longitude and latitude \n",
        "fig = crash_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.6,\n",
        "                   figsize=(18,11),c=\"Accident_Severity_numerical\", cmap=plt.get_cmap(\"inferno\"), \n",
        "                   colorbar=True,)\n",
        "\n",
        "# remove irrelevant columns\n",
        "crash_df = crash_df.drop(columns=['Longitude', 'Latitude', 'Accident_Severity_numerical'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtezT-PJZ1_7"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Question: What does -1 refer to? Why does it occur so often?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7OZ_8ekxd9N"
      },
      "outputs": [],
      "source": [
        "# Change -1 values in 2nd_Road_Type to \"Not a junction\" if the \"Junction Detail\" columns let's us see that there was no junction\n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Not at junction or within 20 metres') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Other junction') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Private drive or entrance') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'T or staggered junction') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'\n",
        "\n",
        "# Plot the value distriburion once more for the \"2nd_Road_Class\" column\n",
        "ax = sns.countplot(crash_df['2nd_Road_Class'], palette=\"pastel\", edgecolor=\".6\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMpqq1sAQCbN"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "Great! For now we are done with the Pre-Processing and we can start with the Feature Engineering\n",
        "\n",
        "**Main points learned:**\n",
        "\n",
        "1. Reading data may come with format problems\n",
        "2. Most of the time, the dataset needs explanation\n",
        "3. There will be inconsistencies / errors in the data; they can be partly fixed with common sense\n",
        "4. There will be no perfect dataset, unless it comes from a textbook\n",
        "5. When leading Data Scientists, be aware that machine learning requires a lot of data Data Cleanining / Pre-Processing, which can be up to 90% of the total time needed for the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_6vE50g5ktT"
      },
      "source": [
        "**Where we stand in the process & key tasks for you as a manager:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step2_sh.png?alt=media&token=48e29cfc-7af7-4f8f-b561-23d404562a0f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odBmfJz1Gtrc"
      },
      "source": [
        "# Step 3: Feature engineering / Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step3_before_sh.png?alt=media&token=e0c2a3a2-4f12-4f8f-8dce-51c07cf40423)\n",
        "\n",
        "Feature Engineering / EDA is an approach to \n",
        "- analyze datasets to summarize their main characteristics \n",
        "- use domain knowledge of the data to create features that make machine learning algorithms work\n",
        "\n",
        "&emsp;\n",
        "\n",
        "\"*Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.*\"\n",
        "\n",
        "— Andrew Ng, Machine Learning and AI via Brain simulations\n",
        "\n",
        "&emsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YglNiNyY3eyF"
      },
      "source": [
        "## 3.1 Target Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfMZCWLnPXza"
      },
      "source": [
        "Let us start by looking at our target variable - the severity of the crash. Based on the severity we would send a different type of ambulance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1F_iw2EHMyS"
      },
      "outputs": [],
      "source": [
        "# Display count of crashed for each severity\n",
        "ax = sns.countplot(crash_df['Accident_Severity'], palette=\"pastel\", edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyAPW2yhCQ3"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "## 3.2 Features\n",
        "\n",
        "Diving deeper, we now compare different predictors (e.g. *speed*) to the target variable (the *accident severity* in our case) to see if we intuitively think that there is a correlation or even causation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBLFmWVLxwjR"
      },
      "outputs": [],
      "source": [
        "#@title After running this cell manually, it will auto-run if you change the selected value. { run: \"auto\" }\n",
        "\n",
        "crash_details = \"Junction_Detail\" #@param [\"Speed_limit\", \"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\", \"Special_Conditions_at_Site\", \"Carriageway_Hazards\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"]\n",
        "\n",
        "accident_counts = (crash_df.groupby([crash_details])[\"Accident_Severity\"]\n",
        "                     .value_counts(normalize=True)\n",
        "                     .rename('percentage')\n",
        "                     .mul(100)\n",
        "                     .reset_index()\n",
        "                     .sort_values(crash_details))\n",
        "p = sns.barplot(x=\"Accident_Severity\", y=\"percentage\", hue=crash_details, palette=\"pastel\", data=accident_counts, edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "p.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
        "p = plt.setp(p.get_xticklabels(), rotation=40)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfBam33mX31q"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Question: Form your own hypotheses. First, reason about what you inuitively think is the best predictor out of Speed_limit, Urban_or_Rural_Area or Did_Police_Officer_Attend_Scene_of_Accident?** \n",
        "\n",
        "**Task: Now we want you to test your hypotheses by exploring the data reality and finding the best predictor** \n",
        "\n",
        "It's time for you to test your hypotheses and find the best predictor. You can explore the correlation of each feature with the target variable in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC3v01ZYW0hz"
      },
      "outputs": [],
      "source": [
        "#@title After running this cell manually, it will auto-run if you change the selected value. { run: \"auto\" }\n",
        "\n",
        "crash_details = \"Urban_or_Rural_Area\" #@param [\"Speed_limit\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"]\n",
        "\n",
        "accident_counts = (crash_df.groupby([crash_details])[\"Accident_Severity\"]\n",
        "                     .value_counts(normalize=True)\n",
        "                     .rename('percentage')\n",
        "                     .mul(100)\n",
        "                     .reset_index()\n",
        "                     .sort_values(crash_details))\n",
        "p = sns.barplot(x=\"Accident_Severity\", y=\"percentage\", hue=crash_details, palette=\"pastel\", data=accident_counts, edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "p.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
        "p = plt.setp(p.get_xticklabels(), rotation=40)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMRzY9fs7Gya"
      },
      "source": [
        "\n",
        "\n",
        "&emsp;\n",
        "\n",
        "As we can see there are definitely better and worse features for predicting our target but not THE one feature that is sufficient.\n",
        "\n",
        "**Question: What other features can you think of that could be important but that we cannot find in the dataset?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YNxTiIK9fS6"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "**Main points learned:**\n",
        "1. During the analysis, additional questions and problems always come up\n",
        "2. Analyzing the data without a guiding business question takes an exponential amount of time\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**Where we stand in the process & key tasks for you as a manager:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step3_sh.png?alt=media&token=322e5986-5327-4920-95c4-7802d373ac76)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pk-32_kIw9F"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Question: One more thing! What about our target variable? How is it different from what we want to predict?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtvgPG1ZFoui"
      },
      "outputs": [],
      "source": [
        "crash_df[\"Accident_Severity\"].replace(\"Serious\", \"Fatal\", inplace=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b1Od57J9ldJ"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "# Step 4: Modelling\n",
        "\n",
        "So far, we have done no machine learning at all, but have prepared us and the data set for the machine learning step of the process. The actual machine learning makes up for a rather small portion of the overall effort.\n",
        "\n",
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step4_before_sh.png?alt=media&token=7a366b1b-ba05-480b-876b-6aa5cf770d5a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_BpBnsw0qlI"
      },
      "source": [
        "## 4.1 Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7e0z3cPBzvv"
      },
      "source": [
        "### 4.1.1 Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sQdVnIG-ULJ"
      },
      "outputs": [],
      "source": [
        "#OneHotEncoded: \"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\", \"Special_Conditions_at_Site\", \"Carriageway_Hazards\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"\n",
        "#crash_df = pd.get_dummies(crash_df, prefix=[\"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Physical_Facilities\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Weather_Conditions\"], columns=[\"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Physical_Facilities\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Weather_Conditions\"])\n",
        "\n",
        "#Categorize Strings: Accident_Severity\n",
        "#le = LabelEncoder()\n",
        "#crash_df['Accident_Severity'] = le.fit_transform(crash_df['Accident_Severity'].tolist())\n",
        "\n",
        "# Normalize nunerical values - squeeze them between range 0 and 1\n",
        "#scaler = MinMaxScaler()\n",
        "#scaler.fit(crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]])\n",
        "#crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]] = scaler.transform(crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]])\n",
        "\n",
        "# Sperate dataset into predictors and target set\n",
        "#y = crash_df[\"Accident_Severity\"]\n",
        "#X = crash_df.drop(columns=['Accident_Severity'])\n",
        "X = pd.read_csv('/content/Quantgeist/X.csv')\n",
        "y = pd.read_csv('/content/Quantgeist/y.csv')\n",
        "X_colnames = list(X.columns)\n",
        "\n",
        "# Oversample\n",
        "#sm = SMOTE(random_state=42)\n",
        "#X, y = sm.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3RQD1ToH5Pg"
      },
      "source": [
        "\n",
        "1.   **Encoding of variables**\n",
        "\n",
        "Encoding is the transformation of categorical variables to binary or numerical counterparts.\n",
        "\n",
        "\n",
        "2.  **Feature scaling**\n",
        "\n",
        "Most of the machine learning algorithms use the Eucledian distance between two data points in their computations. As our features vary in magnitudes, units and range, the results would vary greatly between different units. In our use case this is the case for the Number_of_Vehicles which ranges form 1 to 23 while the variable Speed_limit ranges from 20 to 90. The features with high magnitudes would weigh in a lot more in the calculations than features with low magnitudes.\n",
        "\n",
        "There are several ways to deal with feature scaling, like normalization and standardization. We will use a simple MinMax scaling that squeezes all numerical features within a 0 to 1 range.\n",
        "\n",
        "\n",
        "3.   **Balancing Dataset**\n",
        "\n",
        "As we have a lot less datapoints with serious or fatal accident severity (only 18%) than datapoints with slight severity, a model that would predict all crashes as slight severity accidents would already achieve 82% accuracy. We will go into more details later in the section **Model Evaluation**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drUKSW3CNMwR"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 4.1.2 Train Test Split\n",
        "\n",
        "This is the golden rule of machine learning: **Never train your model on your test data**. Unfortunately only few people adhere to this strictly enough. Usually, you should hold out 20% of your data for testing.\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/train_test_sh.png?alt=media&token=594998cc-2f41-4cfd-a336-5dd4fbadedcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_5i2riYGLwi"
      },
      "outputs": [],
      "source": [
        "# Split data 80:20 in traing, test data\n",
        "sss = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=1)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDXT0IPPNZub"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "## 4.2 Building the Model\n",
        "\n",
        "In this chapter, we will compare different types of machine learning algorithms. To do that we set up and train each of the different algorithms on the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML-g27QfmRYj"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "### 4.2.1 Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9zVFZuKmXG0"
      },
      "outputs": [],
      "source": [
        "# Set up and train model\n",
        "model = Sequential()\n",
        "model.add(Dense(90,input_shape=(80,)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(32))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', ])\n",
        "\n",
        "model.fit(X_train, y_train ,batch_size=128,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kekoSkFmNik_"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 4.2.2 Random Forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbIokzYbdf9d"
      },
      "source": [
        "First, let's set up the algorithm with the parameter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2eLdMEZQo0M"
      },
      "outputs": [],
      "source": [
        "# Set up\n",
        "rf = RandomForestClassifier(verbose=2, random_state=42, n_jobs = -1, class_weight=\"balanced_subsample\", n_estimators=250, max_depth=None, min_samples_split=2, min_samples_leaf=1, bootstrap=True, max_features=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRHGAhPdo8N"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Task: Now we want you to write the code to actually build a machine learning model, in this case a random forest. Please write the python code below for training a machine learning model. As a hint you can look how we did this for the Neural Network. The library you are calling upon is called rf (short for random forest) and the method you wanto to use is fit(). You will see this also follows the same syntax logic you used at the very beginning in your first coding challenge.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZw9lWzpeRdx"
      },
      "outputs": [],
      "source": [
        "#Your code goes here\n",
        "trained_rf = rf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWFe_StZHIb6"
      },
      "source": [
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step4_sh.png?alt=media&token=807883aa-c115-4d0d-ad5b-1d61ccfc825e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwMVXk4_QAcM"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "# Step 5: Model Evaluation\n",
        "\n",
        "**Where we stand in the process:**\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step5_before_sh.png?alt=media&token=ed203fd7-1441-4a54-8d7d-bd3abad8424a)\n",
        "\n",
        "Now let us compare how well our models actually performed by evalauting the error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZG06ZX_DIcZ"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "### 5.1 Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7oIz7xTrVSd"
      },
      "outputs": [],
      "source": [
        "#Performances on test data\n",
        "def get_measures(y_true, y_pred, th=0.5):\n",
        "  yhat_classes = np.where(y_pred > th, 1, 0)\n",
        "  acc = metrics.accuracy_score(y_true, yhat_classes)\n",
        "  bal_acc = metrics.balanced_accuracy_score(y_true, yhat_classes)\n",
        "  sen = metrics.recall_score(y_true, yhat_classes) \n",
        "  f1 = metrics.f1_score(y_true, yhat_classes)\n",
        "  return [acc, bal_acc, sen, f1]\n",
        "\n",
        "#Run model on test data\n",
        "y_nn = model.predict(X_test)\n",
        "y_rf = rf.predict(X_test)\n",
        "\n",
        "dataset = \"Test\"  \n",
        "overall_peformance_df = pd.DataFrame(columns=['model', 'dataset', 'accuracy', 'balanced accuracy', 'sensitivity','F1'])\n",
        "overall_peformance_df.loc[len(overall_peformance_df)] = ['Neural network', dataset] + get_measures(y_test, y_nn)\n",
        "overall_peformance_df.loc[len(overall_peformance_df)] = ['Random Forest', dataset] + get_measures(y_test, y_rf)\n",
        "overall_peformance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GENe6iAXvN4j"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "Can we already pick a winner? Let us look more specifically at the confusion matrix for each model.\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/confusion_042020.png?alt=media&token=5d4c0c03-50ef-406a-87e1-780b70326559)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrBdyqAPyROO"
      },
      "outputs": [],
      "source": [
        "# print confusion matrix\n",
        "#@title After running this cell manually, it will auto-run if you change the selected value. { run: \"auto\" }\n",
        "\n",
        "model_details = \"Random Forest\" #@param [\"Deep Neural Network\", \"Random Forest\"]\n",
        "if model_details == \"Deep Neural Network\":\n",
        "  model_pred = y_nn\n",
        "else:\n",
        "  model_pred = y_rf\n",
        "model_pred = model_pred.round()\n",
        "\n",
        "print(\" Confusion matrix \\n\", metrics.confusion_matrix(y_test, model_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeqlJ92tz8Vg"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "**Question: Which model is better? The one creatd by Deep Neural Network or Random Forest?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHJ4r7Eah3Lc"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "Now let's try to better understand what the model learned. Even though we cannot see how exactly the models arrived at their prediction, we can see which features were deemed most important by the models. \n",
        "So, let's check if you were right in picking the best predictor in the question at the beginning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ3TNSyyiUtQ"
      },
      "outputs": [],
      "source": [
        "feat_importances = pd.Series(rf.feature_importances_, index=X_colnames)\n",
        "feat_importances.nlargest(10).plot(kind='barh')\n",
        "#feat_importances.nsmallest(10).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvTqJeDSjHtq"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "We can see that most of the features that the model deemed important are in line with our inutition, which is a good sign that the model learned something useful. So e.g. Did_Police_Officer_Attend_Scene_of_Accident_No is one of the important features.\n",
        "\n",
        "\n",
        "&emsp;\n",
        "\n",
        "BUT WAIT! **Question: What is the catch with this feature?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAK_1dEIoDeO"
      },
      "source": [
        "\n",
        "We do not have this information at the time that the prediciton is made, which is when the emergency call arrives. Meaning we are not allowed to use this feature as a predictor!! Domain Expertise is required to know this. So it would have been the task of the manager to spot this mistake.\n",
        "\n",
        "Usually, we would have to drop this feature at the beginning and do the whole process again with unforeseeable outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrl4CDTDRYD"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 5.2 Final Decision\n",
        "\n",
        "**Question: Even more important than the actual metric is the question whether the model is good enough for us to use it in production?**\n",
        "\n",
        "The answer is: This depends on our baseline, the current process we use to judge which ambulance to send and how good this current process is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPQTMGQVHZvh"
      },
      "source": [
        "**Where we stand in the process & key tasks for you as a manager:**\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step5_sh.png?alt=media&token=c9fdcbcf-9f78-467c-8797-26d8110ace9d)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Data_for_Managers.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
