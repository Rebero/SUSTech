{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccRAVackyKLs"
      },
      "source": [
        "\n",
        "# Python机器学习全程模拟教程\n",
        "## 作者：雷内·埃伯（音译）\n",
        "\n",
        "**保密文件-未经作者书面许可，不得复制或转发**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83OVH_lm0SwI"
      },
      "source": [
        "# 流程图\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3RJifyUuaGv"
      },
      "source": [
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/process_empty_sh.png?alt=media&token=1168de47-e528-4dbd-aafe-58d2fe992110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wVVc-gbBVub"
      },
      "source": [
        "# 步骤1：起步-业务案例/数据集的介绍\n",
        "\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/ambulance.png?alt=media&token=1494f989-3014-495f-a69b-974275793bda)\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**背景:** 某紧急救援公司负责英国所有的道路事故的紧急处理。作为该公司的一名的管理人员或者首席数据科学家，你有责任：\n",
        "  - 派遣救护车到事发地点（例如：车辆碰撞事故）。\n",
        "  - 将伤者送到离事发地点最近的医院。\n",
        "  - 为伤者提供医疗急救服务。\n",
        "\n",
        "一般有两种不同类型的救护车。它们能提供不同范围的医疗急救服务。\n",
        "\n",
        "&emsp;\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/ambulances.png?alt=media&token=79e882b8-ad9f-4ebb-91df-ec06eeb1eb06)\n",
        "\n",
        "&emsp;\n",
        "\n",
        "左图的救护车上只有最基础的医疗设备，适用于救助轻伤病人。\n",
        "\n",
        "右图的救护车可视为是医院的延伸，因为车上有医生和成熟的医疗设备。即使是复杂的医疗操作，如手术，也可以在去医院的路上进行。\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**目标:** 你需要派遣一辆匹配的救护车，因为：\n",
        "  - 这两种类型的救护车的数量都很有限。\n",
        "  - 只有在发生严重或致命事故时，才需派遣配备医疗队的救护车，且该车费用高昂得多。\n",
        "&emsp;\n",
        "\n",
        "**问题:** 在了解事故的严重程度前，你必须决定派遣哪种类型的救护车。\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**解决思路:** 我们希望根据事故报道的可用的数据来预测事故的严重程度。根据这些数据点，我们想要创建一个事故严重程度的预测机制，这样在接到紧急电话后，就可以精准地确定派遣哪种类型的救护车。\n",
        "\n",
        "我们将使用可公开访问的英国车辆碰撞事故数据库来创建预测模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnCZegCN0mcO"
      },
      "source": [
        "**问题：这是哪种类型的机器学习**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/types_sh.png?alt=media&token=db812890-c7f9-4856-b0ed-4db12f30c57a)\n",
        "\n",
        "**作为一名管理人员，目前在流程中所处的位置和关键任务是:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step1_sh.png?alt=media&token=01334c65-67de-4398-9067-ab3706caa87f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhFfKOi4BGds"
      },
      "source": [
        "# 步骤2：预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0-pYF6KIibd"
      },
      "source": [
        "**目前在流程中所处的位置:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step2_before_sh.png?alt=media&token=8a3c8adb-04fd-491f-b0c7-a158026f5e69)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdE7agfm2DXx"
      },
      "source": [
        "首先，我们会导入将在此研讨会中使用的“库”（例如：熊猫数据集）。这些“库”包含了我们将要使用的所有功能。（例如：计算平均值、可视化数据）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwPDwpXrzjzh"
      },
      "outputs": [],
      "source": [
        "# Loading libraries\n",
        "import os\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn as sklearn\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "sns.set()\n",
        "\n",
        "# Connect to data source\n",
        "! git clone https://github.com/Rebero/Quantgeist\n",
        "time.sleep(10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmNkMpoY2jMK"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "我们已经准备好导入数据集："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgfAYCAd2hut"
      },
      "outputs": [],
      "source": [
        "# import data from a comma seperated files (csv) file  \n",
        "crash_df = pd.read_csv('Quantgeist/accidents_dataset_corrected.csv')\n",
        "\n",
        "crash_df[\"Accident_Severity_numerical\"] = crash_df[\"Accident_Severity\"]\n",
        "# Read from file with number descriptions and replace number with description where possible\n",
        "xls = pd.ExcelFile('Quantgeist/Road-Accident-Safety-Data-Guide.xls')\n",
        "for name in xls.sheet_names: \n",
        "  if name != \"Introduction\" and name != \"Export Variables\":\n",
        "    names_df = pd.read_excel(xls, name)\n",
        "    if name == \"Ped Cross - Human\":\n",
        "      name = \"Pedestrian_Crossing-Human_Control\"\n",
        "    if name == \"Ped Cross - Physical\":\n",
        "      name = \"Pedestrian_Crossing-Physical_Facilities\"\n",
        "    if name == \"Weather\":\n",
        "      name = \"Weather_Conditions\"\n",
        "    if name == \"Road Surface\":\n",
        "      name = \"Road_Surface_Conditions\"\n",
        "    if name == \"Urban Rural\":\n",
        "      name = \"Urban_or_Rural_Area\"\n",
        "    if name == \"Police Officer Attend\":\n",
        "      name = \"Did_Police_Officer_Attend_Scene_of_Accident\"\n",
        "    name = name.replace(\" \", \"_\")\n",
        "    if name in list(crash_df.columns):\n",
        "      rename_dict = names_df.set_index('code').to_dict()['label']\n",
        "      crash_df[name] = crash_df[name].replace(rename_dict)\n",
        "\n",
        "# remove irrelevant columns\n",
        "crash_df = crash_df.drop(columns=['Location_Easting_OSGR', 'Location_Northing_OSGR', \"Local_Authority_(District)\", \"Local_Authority_(Highway)\", \"LSOA_of_Accident_Location\", \"Police_Force\", \"Accident_Index\", \"1st_Road_Number\",\t\"2nd_Road_Number\", \"Date\",\t\"Time\",\t\"Special_Conditions_at_Site\", \"Carriageway_Hazards\", \"Pedestrian_Crossing-Human_Control\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jkAKNuw_CRD"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "想要要获得数据集的第一个汇总，可以查看数据的维度，例如，碰撞事故的频次和报道频次。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg7AkA4D_M81"
      },
      "outputs": [],
      "source": [
        "# dataframe.size returns the size of our dataset\n",
        "crash_df.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZL8BMF7KRCG"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**T任务：我希望你们自己编写代码** 目的是可以看到实际的数据。有一种可以用到的方法称为“head”。请记住我们的数据集名称为“crash_df”。你可以用类似的方法，看看我们上面的数据集是如何生成右上方的crash_df数据集的。 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS-B8-jlB_hI"
      },
      "outputs": [],
      "source": [
        "#你的代码在这里\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wPbPBoUIWLi"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "如果我们将每次事故的经纬度值映射到一个空的坐标系上。**问题：你将会看到什么?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPw36f34IShb"
      },
      "outputs": [],
      "source": [
        "# lets map the longitude and latitude \n",
        "fig = crash_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.6,\n",
        "                   figsize=(18,11),c=\"Accident_Severity_numerical\", cmap=plt.get_cmap(\"inferno\"), \n",
        "                   colorbar=True,)\n",
        "\n",
        "# remove irrelevant columns\n",
        "crash_df = crash_df.drop(columns=['Longitude', 'Latitude', 'Accident_Severity_numerical'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtezT-PJZ1_7"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**问题：-1指的是什么？为什么发生频率那么高?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7OZ_8ekxd9N"
      },
      "outputs": [],
      "source": [
        "# Change -1 values in 2nd_Road_Type to \"Not a junction\" if the \"Junction Detail\" columns let's us see that there was no junction\n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Not at junction or within 20 metres') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Other junction') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'Private drive or entrance') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'  \n",
        "crash_df.loc[(crash_df[\"Junction_Detail\"] == 'T or staggered junction') & (crash_df[\"2nd_Road_Class\"] == -1), '2nd_Road_Class'] = 'Not a junction'\n",
        "\n",
        "# Plot the value distriburion once more for the \"2nd_Road_Class\" column\n",
        "ax = sns.countplot(crash_df['2nd_Road_Class'], palette=\"pastel\", edgecolor=\".6\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMpqq1sAQCbN"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "很棒！现在我们已经完成了预处理工作，我们可以开始特征工程了。\n",
        "\n",
        "**小结:**\n",
        "\n",
        "1. 读取数据可能会出现格式问题。\n",
        "2. 大多数时候，需要分析数据集。\n",
        "3. 数据中会出现不匹配/错误现象；部分可以用常识解决。\n",
        "4. 不存在任何完美的数据集，除非是教科书中描述的理想状态。\n",
        "5. 引导数据科学家时，要意识到机器学习需要大量的数据清理/预算处理，这会占项目总耗时长的90%。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_6vE50g5ktT"
      },
      "source": [
        "**我们作为一名管理人员，所处在流程中的位置以及关键任务是:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step2_sh.png?alt=media&token=48e29cfc-7af7-4f8f-b561-23d404562a0f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odBmfJz1Gtrc"
      },
      "source": [
        "# 步骤3：特征工程/探索性数据分析(EDA)\n",
        "\n",
        "**我们处在流程中的这一步:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step3_before_sh.png?alt=media&token=e0c2a3a2-4f12-4f8f-8dce-51c07cf40423)\n",
        "\n",
        "特征工程 / EDA是一种\n",
        "- 通过分析数据集并总结其主要特征。\n",
        "- 使用数据领域知识来促使机器学习算法工作的方法。\n",
        "\n",
        "&emsp;\n",
        "\n",
        "\"*特征的总结是困难的、耗时的，需要具备专业知识。应用机器学习基本上属于特征工程。*\"\n",
        "\n",
        "— Andrew Ng, 《模拟大脑进行机器学习人工智能》\n",
        "\n",
        "&emsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YglNiNyY3eyF"
      },
      "source": [
        "## 3.1 目标变量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfMZCWLnPXza"
      },
      "source": [
        "我们先来看目标变量——碰撞事故的严重程度。\n",
        "我们会根据严重程度派遣不同类型的救护车。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1F_iw2EHMyS"
      },
      "outputs": [],
      "source": [
        "# 显示每个严重程度的次数\n",
        "ax = sns.countplot(crash_df['Accident_Severity'], palette=\"pastel\", edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyAPW2yhCQ3"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "## 3.2 特征\n",
        "\n",
        "Diving deeper, we now compare different predictors (e.g. *speed*) to the target variable (the *accident severity* in our case) to see if we intuitively think that there is a correlation or even causation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBLFmWVLxwjR"
      },
      "outputs": [],
      "source": [
        "feature_selector = widgets.Dropdown(\n",
        "    options=[\"Speed_limit\", \"Day_of_Week\",\"1st_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Physical_Facilities\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"],\n",
        "    description='Feature:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "crash_details = \"Speed_limit\"\n",
        "feature_selected = widgets.Output()\n",
        "\n",
        "def feature_selector_handler(change):\n",
        "    clear_output(wait=True)\n",
        "    feature_selected.clear_output()\n",
        "    display(feature_selector) \n",
        "    with feature_selected:\n",
        "        crash_details = feature_selector.value\n",
        "        accident_counts = (crash_df.groupby([crash_details])[\"Accident_Severity\"]\n",
        "                             .value_counts(normalize=True)\n",
        "                             .rename('percentage')\n",
        "                             .mul(100)\n",
        "                             .reset_index()\n",
        "                             .sort_values(crash_details))\n",
        "        p = sns.barplot(x=\"Accident_Severity\", y=\"percentage\", hue=crash_details, palette=\"pastel\", data=accident_counts, edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "        p.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
        "        p = plt.setp(p.get_xticklabels(), rotation=40) \n",
        "        \n",
        "feature_selector.observe(feature_selector_handler, names=\"value\")\n",
        "display(feature_selector) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfBam33mX31q"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**问题：提出自己的假设。首先，在超速驾驶、城乡差异、警察是否去过事故现场等原因中找出你直觉认为最准确的预测变量。?** \n",
        "\n",
        "**任务：现在我们希望你通过探索数据真实性以及确定最准确的事故原因来检验你的假设。** \n",
        "\n",
        "这时可以检验你的假设，找出最佳预测变量了。你可以在下面的代码中研究每个特性与目标变量的相关性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC3v01ZYW0hz"
      },
      "outputs": [],
      "source": [
        "feature_selector = widgets.Dropdown(\n",
        "    options=[\"Speed_limit\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"],\n",
        "    description='Feature:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "crash_details = \"Urban_or_Rural_Area\"\n",
        "feature_selected = widgets.Output()\n",
        "\n",
        "def feature_selector_handler(change):\n",
        "    clear_output(wait=True)\n",
        "    feature_selected.clear_output()\n",
        "    display(feature_selector) \n",
        "    with feature_selected:\n",
        "        crash_details = feature_selector.value\n",
        "        accident_counts = (crash_df.groupby([crash_details])[\"Accident_Severity\"]\n",
        "                     .value_counts(normalize=True)\n",
        "                     .rename('percentage')\n",
        "                     .mul(100)\n",
        "                     .reset_index()\n",
        "                     .sort_values(crash_details))\n",
        "        p = sns.barplot(x=\"Accident_Severity\", y=\"percentage\", hue=crash_details, palette=\"pastel\", data=accident_counts, edgecolor=\".6\", order = crash_df[\"Accident_Severity\"].value_counts().index)\n",
        "        p.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
        "        p = plt.setp(p.get_xticklabels(), rotation=40)  \n",
        "        \n",
        "feature_selector.observe(feature_selector_handler, names=\"value\")\n",
        "display(feature_selector) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMRzY9fs7Gya"
      },
      "source": [
        "\n",
        "\n",
        "&emsp;\n",
        "\n",
        "正如我们所看到的，肯定有更好的或者更差的特征用以预测我们的目标，但还没找到有足够说服力的特征。\n",
        "\n",
        "**问：你还能想到哪些在数据集中找不到的重要特征呢?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YNxTiIK9fS6"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "**学习要点:**\n",
        "1. 在分析过程中，总有可能出现其他的问题。\n",
        "2. 在没有指导性业务问题的情况下，分析数据需要耗费大量的时间。\n",
        "\n",
        "&emsp;\n",
        "\n",
        "**作为一名管理人员，我们处在流程中的哪个环节，目前的关键任务是什么:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step3_sh.png?alt=media&token=322e5986-5327-4920-95c4-7802d373ac76)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pk-32_kIw9F"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**问题：还有一件事！我们的目标变量是什么？它和我们想要预测的结果有什么不同呢?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtvgPG1ZFoui"
      },
      "outputs": [],
      "source": [
        "crash_df[\"Accident_Severity\"].replace(\"Serious\", \"Fatal\", inplace=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b1Od57J9ldJ"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "# Step 4: Modelling\n",
        "\n",
        "So far, we have done no machine learning at all, but have prepared us and the data set for the machine learning step of the process. The actual machine learning makes up for a rather small portion of the overall effort.\n",
        "\n",
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step4_before_sh.png?alt=media&token=7a366b1b-ba05-480b-876b-6aa5cf770d5a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_BpBnsw0qlI"
      },
      "source": [
        "## 4.1 Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7e0z3cPBzvv"
      },
      "source": [
        "### 4.1.1 Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sQdVnIG-ULJ"
      },
      "outputs": [],
      "source": [
        "#OneHotEncoded: \"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\", \"Special_Conditions_at_Site\", \"Carriageway_Hazards\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\"\n",
        "#crash_df = pd.get_dummies(crash_df, prefix=[\"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Physical_Facilities\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Weather_Conditions\"], columns=[\"Day_of_Week\",\"1st_Road_Class\", \"2nd_Road_Class\",\"Road_Type\", \"Junction_Detail\", \"Junction_Control\", \"Light_Conditions\", \"Road_Surface_Conditions\", \"Pedestrian_Crossing-Physical_Facilities\", \"Urban_or_Rural_Area\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Weather_Conditions\"])\n",
        "\n",
        "#Categorize Strings: Accident_Severity\n",
        "#le = LabelEncoder()\n",
        "#crash_df['Accident_Severity'] = le.fit_transform(crash_df['Accident_Severity'].tolist())\n",
        "\n",
        "# Normalize nunerical values - squeeze them between range 0 and 1\n",
        "#scaler = MinMaxScaler()\n",
        "#scaler.fit(crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]])\n",
        "#crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]] = scaler.transform(crash_df[[\"Speed_limit\", \"Number_of_Vehicles\", \"Number_of_Casualties\"]])\n",
        "\n",
        "# Sperate dataset into predictors and target set\n",
        "#y = crash_df[\"Accident_Severity\"]\n",
        "#X = crash_df.drop(columns=['Accident_Severity'])\n",
        "X = pd.read_csv('Quantgeist/X.csv')\n",
        "y = pd.read_csv('Quantgeist/y.csv')\n",
        "X_colnames = list(X.columns)\n",
        "\n",
        "# Oversample\n",
        "#sm = SMOTE(random_state=42)\n",
        "#X, y = sm.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3RQD1ToH5Pg"
      },
      "source": [
        "\n",
        "1.   **Encoding of variables**\n",
        "\n",
        "Encoding is the transformation of categorical variables to binary or numerical counterparts.\n",
        "\n",
        "\n",
        "2.  **Feature scaling**\n",
        "\n",
        "Most of the machine learning algorithms use the Eucledian distance between two data points in their computations. As our features vary in magnitudes, units and range, the results would vary greatly between different units. In our use case this is the case for the Number_of_Vehicles which ranges form 1 to 23 while the variable Speed_limit ranges from 20 to 90. The features with high magnitudes would weigh in a lot more in the calculations than features with low magnitudes.\n",
        "\n",
        "There are several ways to deal with feature scaling, like normalization and standardization. We will use a simple MinMax scaling that squeezes all numerical features within a 0 to 1 range.\n",
        "\n",
        "\n",
        "3.   **Balancing Dataset**\n",
        "\n",
        "As we have a lot less datapoints with serious or fatal accident severity (only 18%) than datapoints with slight severity, a model that would predict all crashes as slight severity accidents would already achieve 82% accuracy. We will go into more details later in the section **Model Evaluation**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drUKSW3CNMwR"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 4.1.2 Train Test Split\n",
        "\n",
        "This is the golden rule of machine learning: **Never train your model on your test data**. Unfortunately only few people adhere to this strictly enough. Usually, you should hold out 20% of your data for testing.\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/train_test_sh.png?alt=media&token=594998cc-2f41-4cfd-a336-5dd4fbadedcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_5i2riYGLwi"
      },
      "outputs": [],
      "source": [
        "# Split data 80:20 in traing, test data\n",
        "sss = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=1)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDXT0IPPNZub"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "## 4.2 Building the Model\n",
        "\n",
        "In this chapter, we will compare different types of machine learning algorithms. To do that we set up and train each of the different algorithms on the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML-g27QfmRYj"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "### 4.2.1 Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9zVFZuKmXG0"
      },
      "outputs": [],
      "source": [
        "# Set up and train model\n",
        "model = Sequential()\n",
        "model.add(Dense(90,input_shape=(80,)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(32))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', ])\n",
        "\n",
        "model.fit(X_train, y_train ,batch_size=128,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kekoSkFmNik_"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 4.2.2 Random Forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbIokzYbdf9d"
      },
      "source": [
        "First, let's set up the algorithm with the parameter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2eLdMEZQo0M"
      },
      "outputs": [],
      "source": [
        "# Set up\n",
        "rf = RandomForestClassifier(verbose=2, random_state=42, n_jobs = -1, class_weight=\"balanced_subsample\", n_estimators=150, max_depth=None, min_samples_split=2, min_samples_leaf=1, bootstrap=True, max_features=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkRHGAhPdo8N"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "**Task: Now we want you to write the code to actually build a machine learning model, in this case a random forest. Please write the python code below for training a machine learning model. As a hint you can look how we did this for the Neural Network. The library you are calling upon is called rf (short for random forest) and the method you wanto to use is fit(). You will see this also follows the same syntax logic you used at the very beginning in your first coding challenge.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZw9lWzpeRdx"
      },
      "outputs": [],
      "source": [
        "#Your code goes here\n",
        "trained_rf = rf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWFe_StZHIb6"
      },
      "source": [
        "**Where we stand in the process:**\n",
        "\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step4_sh.png?alt=media&token=807883aa-c115-4d0d-ad5b-1d61ccfc825e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwMVXk4_QAcM"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "# Step 5: Model Evaluation\n",
        "\n",
        "**Where we stand in the process:**\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step5_before_sh.png?alt=media&token=ed203fd7-1441-4a54-8d7d-bd3abad8424a)\n",
        "\n",
        "Now let us compare how well our models actually performed by evalauting the error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZG06ZX_DIcZ"
      },
      "source": [
        "&emsp;\n",
        "\n",
        "### 5.1 Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7oIz7xTrVSd"
      },
      "outputs": [],
      "source": [
        "#Performances on test data\n",
        "def get_measures(y_true, y_pred, th=0.5):\n",
        "  yhat_classes = np.where(y_pred > th, 1, 0)\n",
        "  acc = metrics.accuracy_score(y_true, yhat_classes)\n",
        "  bal_acc = metrics.balanced_accuracy_score(y_true, yhat_classes)\n",
        "  sen = metrics.recall_score(y_true, yhat_classes) \n",
        "  f1 = metrics.f1_score(y_true, yhat_classes)\n",
        "  return [acc, bal_acc, sen, f1]\n",
        "\n",
        "#Run model on test data\n",
        "y_nn = model.predict(X_test)\n",
        "y_rf = rf.predict(X_test)\n",
        "\n",
        "dataset = \"Test\"  \n",
        "overall_peformance_df = pd.DataFrame(columns=['model', 'dataset', 'accuracy', 'balanced accuracy', 'sensitivity','F1'])\n",
        "overall_peformance_df.loc[len(overall_peformance_df)] = ['Neural network', dataset] + get_measures(y_test, y_nn)\n",
        "overall_peformance_df.loc[len(overall_peformance_df)] = ['Random Forest', dataset] + get_measures(y_test, y_rf)\n",
        "overall_peformance_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GENe6iAXvN4j"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "Can we already pick a winner? Let us look more specifically at the confusion matrix for each model.\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/confusion_042020.png?alt=media&token=5d4c0c03-50ef-406a-87e1-780b70326559)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrBdyqAPyROO"
      },
      "outputs": [],
      "source": [
        "# print confusion matrix\n",
        "#@title After running this cell manually, it will auto-run if you change the selected value. { run: \"auto\" }\n",
        "\n",
        "model_details = \"Random Forest\" #@param [\"Deep Neural Network\", \"Random Forest\"]\n",
        "if model_details == \"Deep Neural Network\":\n",
        "  model_pred = y_nn\n",
        "else:\n",
        "  model_pred = y_rf\n",
        "model_pred = model_pred.round()\n",
        "\n",
        "print(\" Confusion matrix \\n\", metrics.confusion_matrix(y_test, model_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeqlJ92tz8Vg"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "**Question: Which model is better? The one creatd by Deep Neural Network or Random Forest?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHJ4r7Eah3Lc"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "Now let's try to better understand what the model learned. Even though we cannot see how exactly the models arrived at their prediction, we can see which features were deemed most important by the models. \n",
        "So, let's check if you were right in picking the best predictor in the question at the beginning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ3TNSyyiUtQ"
      },
      "outputs": [],
      "source": [
        "feat_importances = pd.Series(rf.feature_importances_, index=X_colnames)\n",
        "feat_importances.nlargest(10).plot(kind='barh')\n",
        "#feat_importances.nsmallest(10).plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvTqJeDSjHtq"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "We can see that most of the features that the model deemed important are in line with our inutition, which is a good sign that the model learned something useful. So e.g. Did_Police_Officer_Attend_Scene_of_Accident_No is one of the important features.\n",
        "\n",
        "\n",
        "&emsp;\n",
        "\n",
        "BUT WAIT! **Question: What is the catch with this feature?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAK_1dEIoDeO"
      },
      "source": [
        "\n",
        "We do not have this information at the time that the prediciton is made, which is when the emergency call arrives. Meaning we are not allowed to use this feature as a predictor!! Domain Expertise is required to know this. So it would have been the task of the manager to spot this mistake.\n",
        "\n",
        "Usually, we would have to drop this feature at the beginning and do the whole process again with unforeseeable outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrl4CDTDRYD"
      },
      "source": [
        "\n",
        "&emsp;\n",
        "\n",
        "### 5.2 Final Decision\n",
        "\n",
        "**Question: Even more important than the actual metric is the question whether the model is good enough for us to use it in production?**\n",
        "\n",
        "The answer is: This depends on our baseline, the current process we use to judge which ambulance to send and how good this current process is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPQTMGQVHZvh"
      },
      "source": [
        "**Where we stand in the process & key tasks for you as a manager:**\n",
        "![alt text](https://firebasestorage.googleapis.com/v0/b/heccoding.appspot.com/o/step5_sh.png?alt=media&token=c9fdcbcf-9f78-467c-8797-26d8110ace9d)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Data_for_Managers.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
